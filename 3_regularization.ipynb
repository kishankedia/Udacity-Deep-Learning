{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep Learning</h1>\n",
    "<h2>Assignment 3</h2><br/>\n",
    "Previously in 2_fullyconnected.ipynb, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First reload the data we generated in 1_notmnist.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (187318, 28, 28), (187318,))\n",
      "('Validation set', (8918, 28, 28), (8918,))\n",
      "('Test set', (8707, 28, 28), (8707,))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pickle_file = 'notMNIST_clean.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (187318, 784), (187318, 10))\n",
      "('Validation set', (8918, 784), (8918, 10))\n",
      "('Test set', (8707, 784), (8707, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "\n",
    "- data as a flat matrix\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem 1</h2><br/>\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor t using nn.l2_loss(t). The right amount of regularization should improve your validation / test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Applying l2 regularization for logistic models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_subset = 10000\n",
    "lambda_val = 0.003 #scaling value for controlling regularization on the weight value\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.constant(train_dataset[:10000, :])\n",
    "    tf_train_labels  = tf.constant(train_labels[:10000, :])\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    #variables\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits = tf.matmul(tf_train_dataset,weights) + biases\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,logits=logits)) + lambda_val*tf.nn.l2_loss(weights)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset,weights) + biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset,weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 26.103168\n",
      "Training accuracy: 11.1%\n",
      "Validation accuracy: 13.8%\n",
      "Loss at step 100: 8.305917\n",
      "Training accuracy: 71.4%\n",
      "Validation accuracy: 69.7%\n",
      "Loss at step 200: 5.868908\n",
      "Training accuracy: 74.8%\n",
      "Validation accuracy: 72.5%\n",
      "Loss at step 300: 4.257320\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 73.9%\n",
      "Loss at step 400: 3.157048\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 75.1%\n",
      "Loss at step 500: 2.398562\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 76.3%\n",
      "Loss at step 600: 1.872542\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 77.3%\n",
      "Loss at step 700: 1.505772\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 78.3%\n",
      "Loss at step 800: 1.248805\n",
      "Training accuracy: 83.4%\n",
      "Validation accuracy: 79.1%\n",
      "Test accuracy: 85.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy increased by around 5 percent when we train the network for the same number of itertions using regularizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Experimenting with higher value of lambda </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying l2 regularization for logistic models\n",
    "train_subset = 10000\n",
    "lambda_val = 3 # We have taken a very high value\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.constant(train_dataset[:10000, :])\n",
    "    tf_train_labels  = tf.constant(train_labels[:10000, :])\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    #variables\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits = tf.matmul(tf_train_dataset,weights) + biases\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,logits=logits)) + lambda_val*tf.nn.l2_loss(weights)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset,weights) + biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset,weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 9140.834961\n",
      "Training accuracy: 12.5%\n",
      "Validation accuracy: 13.5%\n",
      "Loss at step 100: 26.667292\n",
      "Training accuracy: 15.5%\n",
      "Validation accuracy: 10.8%\n",
      "Loss at step 200: 22.777790\n",
      "Training accuracy: 15.5%\n",
      "Validation accuracy: 15.4%\n",
      "Loss at step 300: 23.502842\n",
      "Training accuracy: 15.8%\n",
      "Validation accuracy: 11.0%\n",
      "Loss at step 400: 22.101349\n",
      "Training accuracy: 13.0%\n",
      "Validation accuracy: 15.1%\n",
      "Loss at step 500: 26.476952\n",
      "Training accuracy: 15.2%\n",
      "Validation accuracy: 11.1%\n",
      "Loss at step 600: 21.613659\n",
      "Training accuracy: 12.9%\n",
      "Validation accuracy: 15.6%\n",
      "Loss at step 700: 24.193426\n",
      "Training accuracy: 16.9%\n",
      "Validation accuracy: 12.1%\n",
      "Loss at step 800: 23.919769\n",
      "Training accuracy: 8.8%\n",
      "Validation accuracy: 17.9%\n",
      "Test accuracy: 18.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We can see that the accuracy is very bad. It is because more loss is contributed by the regularizing loss that we have added in the loss function than the loss we are getting by comparing logits to labels. Thus, the models is underfitted as it is just trying to choose small weight values in any way to reduce the loss function.\n",
    "\n",
    "Similarly, if we choose very small value of lambda, there will be little to no significance of adding regularizing loss to the loss function and no effect could be seen then </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Applying regularization to 1-hidden layer neural network</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "hidden_layer_nodes = 1000\n",
    "lambda_val = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #create place holders for taking training input\n",
    "    tf_train_dataset = tf.placeholder(dtype=tf.float32,shape=(batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(dtype=tf.float32,shape=(batch_size,num_labels))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    # Variables for computing hidden layer nodes values\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_layer_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_layer_nodes]))\n",
    "    hidden_layer_data = tf.nn.relu(tf.matmul(tf_train_dataset,weights1)+biases1)\n",
    "    \n",
    "    \n",
    "    #variables for computing logits for the output layer\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_layer_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    logits = tf.matmul(hidden_layer_data,weights2)+biases2\n",
    "    \n",
    "    \n",
    "    #calculating loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,logits=logits)) \\\n",
    "            + lambda_val * tf.nn.l2_loss(weights1) + lambda_val * tf.nn.l2_loss(weights2)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset,weights1)+biases1),weights2)+biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset,weights1)+biases1),weights2)+biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 593.787781\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 31.2%\n",
      "Minibatch loss at step 500: 192.214767\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 1000: 112.514793\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1500: 67.780785\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 2000: 40.501095\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 2500: 24.522041\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 3000: 15.073820\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 91.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried multiple lambda values, starting with a higher lambda value and then decreasing the value of lambda by 1/3 its previous value and checked the accuracy. Found 0.01 to better than other values. As lambda is a hyper parameter, experimentation with its values need to be done to get it most optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem 2</h2><br/>\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Taking 1 hidden layer neural network passing only a small subset of the training data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "hidden_layer_nodes = 1000\n",
    "lambda_val = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #create place holders for taking training input\n",
    "    tf_train_dataset = tf.placeholder(dtype=tf.float32,shape=(batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(dtype=tf.float32,shape=(batch_size,num_labels))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    # Variables for computing hidden layer nodes values\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_layer_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_layer_nodes]))\n",
    "    hidden_layer_data = tf.nn.relu(tf.matmul(tf_train_dataset,weights1)+biases1)\n",
    "    \n",
    "    \n",
    "    #variables for computing logits for the output layer\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_layer_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    logits = tf.matmul(hidden_layer_data,weights2)+biases2\n",
    "    \n",
    "    \n",
    "    #calculating loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,logits=logits)) \\\n",
    "            + lambda_val * tf.nn.l2_loss(weights1) + lambda_val * tf.nn.l2_loss(weights2)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset,weights1)+biases1),weights2)+biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset,weights1)+biases1),weights2)+biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 649.880981\n",
      "Minibatch accuracy: 10.2%\n",
      "Minibatch loss at step 500: 186.195587\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1000: 112.919128\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 1500: 68.480408\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2000: 41.530254\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2500: 25.186213\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 3000: 15.274346\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 69.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "small_train_dataset = train_dataset[:2000,:]\n",
    "small_train_labels  = train_labels[:2000]\n",
    "indexes = np.arange(128)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "\n",
    "    np.random.shuffle(indexes)\n",
    "    batch_data = small_train_dataset[indexes]\n",
    "    batch_labels = small_train_labels[indexes]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took 200 images and trained the network, we can see that the model is giving 100% accuracy on our trained dataset. however, for test we are getting only 69% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "<h1>Problem 3</h1><br/>\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides nn.dropout() for that, but you have to make sure it's only inserted during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1 hidden layer neural network with dropout applied to the first layer activations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(187318, 784)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "hidden_layer_nodes = 1000\n",
    "print train_dataset.shape\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #create place holders for taking training input\n",
    "    tf_train_dataset = tf.placeholder(dtype=tf.float32,shape=(batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(dtype=tf.float32,shape=(batch_size,num_labels))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    keep_prob = tf.constant(0.5)\n",
    "    # Variables for computing hidden layer nodes values\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_layer_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_layer_nodes]))\n",
    "    hidden_layer_data = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset,weights1)+biases1),keep_prob)\n",
    "    \n",
    "    \n",
    "    #variables for computing logits for the output layer\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_layer_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    logits = tf.matmul(hidden_layer_data,weights2)+biases2\n",
    "    \n",
    "    \n",
    "    #calculating loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,logits=logits))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset,weights1)+biases1),weights2)+biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset,weights1)+biases1),weights2)+biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 548.001648\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 25.4%\n",
      "Minibatch loss at step 500: 38.624714\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 1000: 13.993723\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 1500: 19.152189\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 2000: 8.705633\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 2500: 5.419950\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 3000: 3.228109\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.1%\n",
      "Test accuracy: 84.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Problem 4</h1><br/>\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1%.\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Applying learning rate decay on a 1 hidden layer neural network</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trying learning rate decay\n",
    "batch_size = 128\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "hidden_layer_nodes = 1000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #create place holders for taking training input\n",
    "    tf_train_dataset = tf.placeholder(dtype=tf.float32,shape=(batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(dtype=tf.float32,shape=(batch_size,num_labels))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    keep_prob = tf.constant(0.5)\n",
    "    # Variables for computing hidden layer nodes values\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_layer_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_layer_nodes]))\n",
    "    hidden_layer_data = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset,weights1)+biases1),keep_prob)\n",
    "    \n",
    "    \n",
    "    #variables for computing logits for the output layer\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_layer_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    logits = tf.matmul(hidden_layer_data,weights2)+biases2\n",
    "    \n",
    "    \n",
    "    global_steps = tf.Variable(0,trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=0.5,global_step=global_steps,decay_rate=0.9,decay_steps=500)\n",
    "    #calculating loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,logits=logits)) + \\\n",
    "        lambda_val * tf.nn.l2_loss(weights1) + lambda_val * tf.nn.l2_loss(weights2)\n",
    "        \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset,weights1)+biases1),weights2)+biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset,weights1)+biases1),weights2)+biases2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added learning rate decay and training the network for more number of iterations will help the network learn better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 842.687805\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 33.5%\n",
      "Minibatch loss at step 500: 199.993484\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1000: 116.190338\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1500: 70.170685\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2000: 41.188198\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2500: 24.621210\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 3000: 15.187391\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.5%\n",
      "Test accuracy: 90.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating a two layer neural network having 1024 nodes in the first layer and 100 nodes on the second layer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trying learning rate decay\n",
    "batch_size = 128\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "hidden1_layer_nodes = 1024\n",
    "hidden2_layer_nodes = 100\n",
    "lambda_val = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #create place holders for taking training input\n",
    "    tf_train_dataset = tf.placeholder(dtype=tf.float32,shape=(batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(dtype=tf.float32,shape=(batch_size,num_labels))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    keep_prob = tf.constant(0.5)\n",
    "    # Variables for computing first hidden layer nodes values\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden1_layer_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden1_layer_nodes]))\n",
    "    hidden1_layer_data = tf.nn.dropout(tf.nn.relu(tf.matmul(tf_train_dataset,weights1)+biases1),keep_prob)\n",
    "    \n",
    "    # Variables for computing second hidden layer nodes values\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_layer_nodes, hidden2_layer_nodes]))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden2_layer_nodes]))\n",
    "    hidden2_layer_data = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden1_layer_data,weights2)+biases2),keep_prob)\n",
    "    \n",
    "    #variables for computing logits for the output layer\n",
    "    weights3 = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_layer_nodes, num_labels]))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    logits = tf.matmul(hidden2_layer_data,weights3)+biases3\n",
    "    \n",
    "    \n",
    "    global_steps = tf.Variable(0,trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=0.5,global_step=global_steps,decay_rate=0.9,decay_steps=5000,)\n",
    "    #calculating loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,logits=logits))\n",
    "        \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    test_hidden2_layer_act = tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset,weights1)+biases1),weights2)+biases2\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_hidden2_layer_act,weights3)+biases3)\n",
    "    \n",
    "    valid_hidden2_layer_act = tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset,weights1)+biases1),weights2)+biases2\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden2_layer_act,weights3)+biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3989.051758\n",
      "Minibatch accuracy: 16.4%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 500: 2.298987\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 10.1%\n",
      "Minibatch loss at step 1000: nan\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 10.1%\n",
      "Minibatch loss at step 1500: nan\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 10.1%\n",
      "Minibatch loss at step 2000: nan\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 10.1%\n",
      "Minibatch loss at step 2500: nan\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.1%\n",
      "Minibatch loss at step 3000: nan\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 10.1%\n",
      "Test accuracy: 10.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strangely in a two layer network that is created above the accuracy of the model is as good as a random guess .\n",
    "<b> It is due to the reason of weight initialization, proper weight initialization methods should be used in deep networks as done below<b/>\n",
    "\n",
    "<h3>Applying better approach for weight initialization </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#appliying proper weight initialization technique\n",
    "#trying learning rate decay\n",
    "batch_size = 128\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "hidden1_layer_nodes = 1024\n",
    "hidden2_layer_nodes = 100\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    #create place holders for taking training input\n",
    "    tf_train_dataset = tf.placeholder(dtype=tf.float32,shape=(batch_size,image_size*image_size))\n",
    "    tf_train_labels = tf.placeholder(dtype=tf.float32,shape=(batch_size,num_labels))\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    \n",
    "    \n",
    "    # Variables for computing first hidden layer nodes values\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden1_layer_nodes],stddev=np.sqrt(2.0 / (image_size * image_size+hidden1_layer_nodes))))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden1_layer_nodes]))\n",
    "    hidden1_layer_data = tf.nn.relu(tf.matmul(tf_train_dataset,weights1)+biases1)\n",
    "    \n",
    "    # Variables for computing second hidden layer nodes values\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_layer_nodes, hidden2_layer_nodes],stddev=np.sqrt(2.0 / (hidden1_layer_nodes+hidden1_layer_nodes))))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden2_layer_nodes]))\n",
    "    hidden2_layer_data = tf.nn.relu(tf.matmul(hidden1_layer_data,weights2)+biases2)\n",
    "    \n",
    "    #variables for computing logits for the output layer\n",
    "    weights3 = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_layer_nodes, num_labels],stddev=np.sqrt(2.0 / (hidden2_layer_nodes+num_labels))))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    logits = tf.matmul(hidden2_layer_data,weights3)+biases3\n",
    "    \n",
    "    \n",
    "    global_steps = tf.Variable(0,trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=0.2,global_step=global_steps,decay_rate=0.8,decay_steps=2000,staircase=True)\n",
    "    #calculating loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,logits=logits)) + \\\n",
    "        lambda_val*tf.nn.l2_loss(weights1) + lambda_val*tf.nn.l2_loss(weights2) + lambda_val*tf.nn.l2_loss(weights3)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    test_hidden2_layer_act = tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset,weights1)+biases1),weights2)+biases2\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_hidden2_layer_act,weights3)+biases3)\n",
    "    \n",
    "    valid_hidden2_layer_act = tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset,weights1)+biases1),weights2)+biases2\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden2_layer_act,weights3)+biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.710467\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 28.1%\n",
      "Minibatch loss at step 5000: 0.694132\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.6%\n",
      "Test accuracy: 93.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 5000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
